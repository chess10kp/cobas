{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3aa98a6",
   "metadata": {},
   "source": [
    "# Multimodal audio+image pipeline\n",
    "\n",
    "This notebook contains a reorganized, split version of the original monolithic pipeline.\n",
    "Cells are grouped by purpose (settings, extraction, audio cleaning, dataset, model, training, tests).\n",
    "Keep this structure for easier refactoring, commenting and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d7a2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import os, glob, subprocess, math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.functional as AF\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9575c486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS\n",
    "RAW_VIDEOS   = \"D:/Downloads/audio/full clips\"     # input videos\n",
    "AUDIO_DIR    = \"D:/Downloads/audio/aud\"   # output 2s wavs\n",
    "IMAGE_DIR    = \"D:/Downloads/audio/img\"  # output frame jpgs\n",
    "\n",
    "\n",
    "SAMPLE_RATE  = 48000\n",
    "CHUNK_SECONDS= 2\n",
    "N_MELS       = 64\n",
    "BATCH_SIZE   = 20\n",
    "EPOCHS       = 15\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Print device info to confirm CUDA availability and details\n",
    "print('Torch CUDA available:', torch.cuda.is_available())\n",
    "print('Using device:', DEVICE)\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        print('CUDA device count:', torch.cuda.device_count())\n",
    "        if torch.cuda.device_count() > 0:\n",
    "            try:\n",
    "                cur = torch.cuda.current_device()\n",
    "                print('CUDA current device index:', cur)\n",
    "                print('CUDA device name:', torch.cuda.get_device_name(cur))\n",
    "            except Exception as e:\n",
    "                print('Could not query CUDA device name:', e)\n",
    "    except Exception as e:\n",
    "        print('CUDA query error:', e)\n",
    "\n",
    "label_map = {\"full\": 0, \"half\": 1, \"empty\": 2}\n",
    "LABELS = {v: k.capitalize() for k, v in label_map.items()}\n",
    "\n",
    "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86dc160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video -> audio/frame helpers (ffmpeg wrappers)\n",
    "def extract_audio(video_path, wav_out, sample_rate=SAMPLE_RATE):\n",
    "    cmd = [\"ffmpeg\", \"-y\", \"-i\", video_path, \"-ar\", str(sample_rate), \"-ac\", \"1\", wav_out]\n",
    "    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
    "\n",
    "def extract_frame(video_path, image_out, timestamp):\n",
    "    cmd = [\"ffmpeg\", \"-y\", \"-i\", video_path, \"-ss\", f\"{timestamp:.3f}\", \"-vframes\", \"1\", image_out]\n",
    "    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
    "\n",
    "def fast_split(video_path, out_audio_dir, out_frame_dir):\n",
    "    base = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    audio_pattern = os.path.join(out_audio_dir, f\"{base}_seg%03d.wav\")\n",
    "    frame_pattern = os.path.join(out_frame_dir, f\"{base}_frame%03d.jpg\")\n",
    "\n",
    "    subprocess.run([\"ffmpeg\", \"-i\", video_path, \"-f\", \"segment\", \"-segment_time\", \"2\", \"-ar\", \"48000\", \"-ac\", \"1\", \"-c:a\", \"pcm_s16le\", audio_pattern], check=True)\n",
    "    subprocess.run([\"ffmpeg\", \"-i\", video_path, \"-vf\", \"fps=0.5\", \"-qscale:v\", \"2\", frame_pattern], check=True)\n",
    "\n",
    "def prepare_dataset_from_videos(raw_videos=RAW_VIDEOS):\n",
    "    video_files = []\n",
    "    video_files.extend(glob.glob(os.path.join(raw_videos, \"*.MOV\")))\n",
    "    video_files.extend(glob.glob(os.path.join(raw_videos, \"*.mov\")))\n",
    "    video_files.extend(glob.glob(os.path.join(raw_videos, \"*.mp4\")))\n",
    "    if not video_files:\n",
    "        print(f\"[WARN] No videos in {raw_videos}\")\n",
    "    for vf in video_files:\n",
    "        print(\"[PROCESSING]\", vf)\n",
    "        fast_split(vf, AUDIO_DIR, IMAGE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "810fc8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE TRANSFORM and audio cleaning constants\n",
    "gray = transforms.Grayscale(num_output_channels=3)\n",
    "transform_image = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    gray,\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# Audio cleaning for a 2s chunk (band-pass + quiet-frame spectral subtraction)\n",
    "SR        = SAMPLE_RATE\n",
    "BAND_LO   = 12000\n",
    "BAND_HI   = 18000\n",
    "NFFT      = 2048\n",
    "HOP       = 512\n",
    "OVERSUB   = 1.2\n",
    "QUIET_PCT = 0.20\n",
    "\n",
    "def _stft(x: torch.Tensor) -> torch.Tensor:\n",
    "    if x.dim() == 2: x = x.squeeze(0)\n",
    "    return torch.stft(x, n_fft=NFFT, hop_length=HOP, win_length=NFFT,\n",
    "                      window=torch.hann_window(NFFT), return_complex=True, center=True)\n",
    "\n",
    "def _istft(S: torch.Tensor, length: int) -> torch.Tensor:\n",
    "    win = torch.hann_window(NFFT, device=S.device, dtype=torch.float32)\n",
    "    try:\n",
    "        y = torch.istft(S, n_fft=NFFT, hop_length=HOP, win_length=NFFT, window=win, length=length, center=True)\n",
    "    except (TypeError, RuntimeError):\n",
    "        y = torch.istft(torch.view_as_real(S), n_fft=NFFT, hop_length=HOP, win_length=NFFT, window=win, length=length, center=True)\n",
    "    return y\n",
    "\n",
    "def bandpass_chunk(waveform: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "    if waveform.dim() == 1:\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "    x = waveform.squeeze(0).to(torch.float32)\n",
    "    S = torch.stft(x, n_fft=NFFT, hop_length=HOP, win_length=NFFT, window=torch.hann_window(NFFT, dtype=torch.float32), return_complex=True, center=True)\n",
    "    freqs = np.fft.rfftfreq(NFFT, d=1.0/sr)\n",
    "    lo = int(np.searchsorted(freqs, BAND_LO))\n",
    "    hi = int(np.searchsorted(freqs, BAND_HI))\n",
    "    lo = max(lo, 0); hi = min(hi, S.shape[0])\n",
    "    mask = torch.zeros_like(S, dtype=torch.bool)\n",
    "    mask[lo:hi, :] = True\n",
    "    S_bp = torch.where(mask, S, torch.zeros_like(S))\n",
    "    y = _istft(S_bp, length=x.numel())\n",
    "    y = y - y.mean()\n",
    "    y = torch.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return y.unsqueeze(0)\n",
    "\n",
    "def spectral_subtract_quiet_frames(waveform: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "    x = waveform.squeeze(0)\n",
    "    Tlen = x.shape[-1]\n",
    "    S = _stft(x)\n",
    "    Mag = S.abs()\n",
    "    Pow = Mag**2\n",
    "    freqs = np.fft.rfftfreq(NFFT, d=1.0/sr)\n",
    "    lo = int(np.searchsorted(freqs, BAND_LO))\n",
    "    hi = int(np.searchsorted(freqs, BAND_HI))\n",
    "    lo = max(lo, 0); hi = min(hi, Mag.shape[0])\n",
    "    band_pow_per_frame = Pow[lo:hi].mean(dim=0)\n",
    "    T_frames = band_pow_per_frame.numel()\n",
    "    k = max(1, int(round(QUIET_PCT * T_frames)))\n",
    "    vals, idxs = torch.topk(-band_pow_per_frame, k)\n",
    "    quiet_mask = torch.zeros_like(band_pow_per_frame, dtype=torch.bool)\n",
    "    quiet_mask[idxs] = True\n",
    "    Npsd = Pow[:, quiet_mask].mean(dim=1, keepdim=True)\n",
    "    Pclean = torch.clamp(Pow - OVERSUB * Npsd, min=0.0)\n",
    "    Mag_clean = torch.sqrt(Pclean + 1e-12)\n",
    "    S_clean = Mag_clean * torch.exp(1j * S.angle())\n",
    "    y_clean = _istft(S_clean, length=Tlen)\n",
    "    y_clean = y_clean - y_clean.mean()\n",
    "    y_clean = torch.nan_to_num(y_clean, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return y_clean.unsqueeze(0)\n",
    "\n",
    "def clean_chunk(waveform: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "    if waveform.dim() == 2 and waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    elif waveform.dim() == 1:\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "    if sr != SR:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, SR)\n",
    "        sr = SR\n",
    "    y_bp = bandpass_chunk(waveform, sr)\n",
    "    y_cl = spectral_subtract_quiet_frames(y_bp, sr)\n",
    "    return y_cl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f85c506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET\n",
    "class AudioImageDataset(Dataset):\n",
    "    def __init__(self, audio_dir, image_dir, label_map, transform_image=None, sample_rate=SAMPLE_RATE, n_mels=N_MELS, use_filters=True):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.label_map = label_map\n",
    "        self.transform_image = transform_image\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.use_filters = use_filters\n",
    "        self.audio_files, self.labels, self.video_ids = [], [], []\n",
    "        for file in os.listdir(audio_dir):\n",
    "            if file.endswith(\".wav\") and \"_seg\" in file:\n",
    "                frame_file = file.replace(\"seg\", \"frame\").replace(\".wav\", \".jpg\")\n",
    "                if not os.path.exists(os.path.join(image_dir, frame_file)):\n",
    "                    continue\n",
    "                base_name = file.split(\"_seg\")[0]\n",
    "                label_str = base_name.split(\"_\")[0].lower()\n",
    "                if label_str in label_map:\n",
    "                    self.audio_files.append(file)\n",
    "                    self.labels.append(label_map[label_str])\n",
    "                    self.video_ids.append(file)\n",
    "        self.mel = T.MelSpectrogram(sample_rate=self.sample_rate, n_fft=1024, hop_length=512, n_mels=self.n_mels)\n",
    "        self.db  = T.AmplitudeToDB()\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file = self.audio_files[idx]\n",
    "        label      = self.labels[idx]\n",
    "        seg_id     = self.video_ids[idx]\n",
    "        a_path = os.path.join(self.audio_dir, audio_file)\n",
    "        waveform, sr = torchaudio.load(a_path)\n",
    "        if self.use_filters:\n",
    "            waveform = clean_chunk(waveform, sr)\n",
    "        else:\n",
    "            if sr != self.sample_rate:\n",
    "                waveform = torchaudio.functional.resample(waveform, sr, self.sample_rate)\n",
    "        spec = self.mel(waveform)\n",
    "        spec = self.db(spec)\n",
    "        spec = (spec - spec.mean()) / (spec.std() + 1e-6)\n",
    "        spec = F.interpolate(spec.unsqueeze(0), size=(224,224), mode=\"bilinear\", align_corners=False)\n",
    "        spec = spec.mean(dim=1)\n",
    "        frame_file = audio_file.replace(\"seg\", \"frame\").replace(\".wav\", \".jpg\")\n",
    "        img_path = os.path.join(self.image_dir, frame_file)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform_image:\n",
    "            img = self.transform_image(img)\n",
    "        return spec, img, label, seg_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930ac41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADERS + MODEL\n",
    "def create_loaders(dataset, batch_size=BATCH_SIZE, num_workers=4):\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size   = len(dataset) - train_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    print(f\"Train samples: {len(train_ds)}, Val samples: {len(val_ds)}\")\n",
    "    return train_loader, val_loader\n",
    "\n",
    "class MultiModalResNet(nn.Module):\n",
    "    def __init__(self, num_classes=3, pretrained_image=True, pretrained_audio=True):\n",
    "        super().__init__()\n",
    "        self.audio_model = models.resnet18(pretrained=pretrained_audio)\n",
    "        self.audio_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.audio_model.fc = nn.Identity()\n",
    "        self.image_model = models.resnet18(pretrained=pretrained_image)\n",
    "        self.image_model.fc = nn.Identity()\n",
    "        self.fc = nn.Linear(512*2, num_classes)\n",
    "    def forward(self, audio, image, return_features=False):\n",
    "        a = self.audio_model(audio)\n",
    "        i = self.image_model(image)\n",
    "        fused = torch.cat([a, i], dim=1)\n",
    "        out = self.fc(fused)\n",
    "        if return_features:\n",
    "            return a, i, fused, out\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c721c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE VISUALIZATION (t-SNE)\n",
    "def visualize_features(model, dataloader, title_suffix=\"\"):\n",
    "    model.eval()\n",
    "    audio_feats, image_feats, fused_feats, labels_all = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for audio, img, labels, _ in dataloader:\n",
    "            audio, img, labels = audio.to(DEVICE), img.to(DEVICE), labels.to(DEVICE)\n",
    "            a, i, fused, _ = model(audio, img, return_features=True)\n",
    "            audio_feats.append(a.cpu()); image_feats.append(i.cpu()); fused_feats.append(fused.cpu())\n",
    "            labels_all.append(labels.cpu())\n",
    "    audio_feats = torch.cat(audio_feats).numpy()\n",
    "    image_feats = torch.cat(image_feats).numpy()\n",
    "    fused_feats = torch.cat(fused_feats).numpy()\n",
    "    labels_all = torch.cat(labels_all).numpy()\n",
    "    print(\"Label counts:\", Counter(labels_all))\n",
    "    n = fused_feats.shape[0]\n",
    "    perpl = min(30, max(2, n//3))\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=perpl)\n",
    "    A2 = tsne.fit_transform(audio_feats)\n",
    "    I2 = tsne.fit_transform(image_feats)\n",
    "    F2 = tsne.fit_transform(fused_feats)\n",
    "    fig, axes = plt.subplots(1,3, figsize=(18,6))\n",
    "    for data2d, name, ax in zip([A2, I2, F2], [\"Audio\", \"Image\", \"Fused\"], axes):\n",
    "        for l in sorted(set(labels_all)):\n",
    "            idx = labels_all == l\n",
    "            ax.scatter(data2d[idx,0], data2d[idx,1], alpha=0.6, s=20, label=LABELS[l])\n",
    "        ax.set_title(f\"{name} Features {title_suffix}\"); ax.legend()\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "# TRAINING\n",
    "def train():\n",
    "    dataset = AudioImageDataset(audio_dir=AUDIO_DIR, image_dir=IMAGE_DIR, label_map=label_map, transform_image=transform_image, sample_rate=SAMPLE_RATE, n_mels=N_MELS, use_filters=True)\n",
    "    if len(dataset) == 0:\n",
    "        print(\"[ERROR] No segments found. Did you run prepare_dataset_from_videos()?\"); return\n",
    "    train_loader, val_loader = create_loaders(dataset, batch_size=BATCH_SIZE)\n",
    "    model = MultiModalResNet(num_classes=len(label_map)).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print(\"Visualizing features BEFORE training...\")\n",
    "    visualize_features(model, val_loader, title_suffix=\"(Before)\")\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for audio, img, labels, seg_ids in train_loader:\n",
    "            audio, img, labels = audio.to(DEVICE), img.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(audio, img)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward(); optimizer.step()\n",
    "            running_loss += loss.item() * audio.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        train_loss = running_loss / max(1,total)\n",
    "        train_acc  = correct / max(1,total)\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        val_preds, val_labels, val_segids = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for audio, img, labels, seg_ids in val_loader:\n",
    "                audio, img, labels = audio.to(DEVICE), img.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(audio, img)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * audio.size(0)\n",
    "                _, preds = outputs.max(1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total   += labels.size(0)\n",
    "                val_preds.extend(preds.cpu().tolist())\n",
    "                val_labels.extend(labels.cpu().tolist())\n",
    "                val_segids.extend(seg_ids)\n",
    "        val_loss /= max(1,val_total)\n",
    "        val_acc   = val_correct / max(1,val_total)\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "              f\"Train Loss: {round(train_loss,4):.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        visualize_features(model, val_loader, title_suffix=f\"(Epoch {epoch+1})\")\n",
    "    print(\"Visualizing features AFTER training...\")\n",
    "    visualize_features(model, val_loader, title_suffix=\"(After)\")\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(\"models\", \"multimodal_model_battery_v3.pth\"))\n",
    "    print(\"Training complete and model saved!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150cf3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROCESSING] D:/Downloads/audio/full clips\\0s_cropped-003_part_000.mov\n",
      "[PROCESSING] D:/Downloads/audio/full clips\\100s_cropped-004_part_000.mov\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "if __name__ == \"__main__\":\n",
    "    prepare_dataset_from_videos(RAW_VIDEOS)\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a51285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward output shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# QUICK SANITY / SHAPE TESTS and seeds\n",
    "import random\n",
    "torch.manual_seed(42); np.random.seed(42); random.seed(42)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(42)\n",
    "# Shape test for model forward pass\n",
    "model = MultiModalResNet(num_classes=3, pretrained_image=False, pretrained_audio=False).to(DEVICE)\n",
    "a = torch.randn(2,1,224,224).to(DEVICE)\n",
    "i = torch.randn(2,3,224,224).to(DEVICE)\n",
    "out = model(a,i)\n",
    "print('Forward output shape:', out.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
