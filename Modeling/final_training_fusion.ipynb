{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. *Band-pass Filter Parameters*\n",
    "- `BAND_LO` and `BAND_HI`: Adjust frequency range (currently 12-18 kHz)\n",
    "- Target different frequency bands for different battery states\n",
    "\n",
    "    - This is essentially how we filter only the frequencies within our range of interest (the ultrasonic waves)\n",
    "\n",
    "## 2. *Spectral Subtraction Parameters*\n",
    "- `QUIET_PCT`: Percentage of quietest frames for noise estimation (currently 20%)\n",
    "- `OVERSUB`: Over-subtraction factor (currently 1.2)\n",
    "    - used to remove noise from our recordings\n",
    "\n",
    "## 3. **STFT Parameters**\n",
    "- `NFFT`: FFT size affects frequency resolution (currently 2048)\n",
    "- `HOP`: Hop length affects time resolution (currently 512)\n",
    "\n",
    "    - an STFT is basically just a time frequency domain representation of the audio waveform, instead of showing up in a magnitude format. \n",
    "\n",
    "## 4. **Mel-spectrogram Parameters**\n",
    "- `N_MELS`: Number of mel bins (currently 64)\n",
    "- `n_fft`: FFT size for mel transform (currently 1024)\n",
    "\n",
    "    - Mel-spectrograms are a way to represent audio signals in a way that aligns with how we perceive sound, specifically focusing on freqs that we can hear more distinctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, subprocess, math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.functional as AF\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# SETTINGS\n",
    "RAW_VIDEOS   = \"raw_videos\"     # input videos\n",
    "AUDIO_DIR    = \"audio_segments\"   # output 2s wavs\n",
    "IMAGE_DIR    = \"recorded_images\"  # output frame jpgs\n",
    "\n",
    "SAMPLE_RATE  = 48000       \n",
    "CHUNK_SECONDS= 2\n",
    "N_MELS       = 64\n",
    "BATCH_SIZE   = 20\n",
    "EPOCHS       = 15\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "label_map = {\"full\": 0, \"half\": 1, \"empty\": 2}\n",
    "LABELS = {v: k.capitalize() for k, v in label_map.items()}\n",
    "\n",
    "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT 2s AUDIO + FRAMES FROM VIDEO\n",
    "def extract_audio(video_path, wav_out, sample_rate=SAMPLE_RATE):\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-y\", \"-i\", video_path,\n",
    "        \"-ar\", str(sample_rate), \"-ac\", \"1\", wav_out\n",
    "    ]\n",
    "    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
    "\n",
    "def extract_frame(video_path, image_out, timestamp):\n",
    "    cmd = [\"ffmpeg\", \"-y\", \"-i\", video_path, \"-ss\", f\"{timestamp:.3f}\", \"-vframes\", \"1\", image_out]\n",
    "    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
    "\n",
    "def split_audio_and_frames(video_path, audio_dir=AUDIO_DIR, image_dir=IMAGE_DIR,\n",
    "                           chunk_sec=CHUNK_SECONDS, sample_rate=SAMPLE_RATE):\n",
    "    base = os.path.splitext(os.path.basename(video_path))[0]   # e.g., full_8\n",
    "    # 1) extract full audio\n",
    "    full_audio = os.path.join(audio_dir, base + \".wav\")\n",
    "    extract_audio(video_path, full_audio, sample_rate=sample_rate)\n",
    "\n",
    "    # 2) load and split into 2s chunks\n",
    "    waveform, sr = torchaudio.load(full_audio)   # [1,T]\n",
    "    if sr != sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, sample_rate)\n",
    "        sr = sample_rate\n",
    "\n",
    "    total_samples = waveform.shape[-1]\n",
    "    chunk_samples = int(chunk_sec * sr)\n",
    "    n_chunks = total_samples // chunk_samples\n",
    "\n",
    "    for i in range(n_chunks):\n",
    "        start = i * chunk_samples\n",
    "        end   = start + chunk_samples\n",
    "        seg   = waveform[:, start:end]                  \n",
    "        seg_name = f\"{base}_seg{i}.wav\"\n",
    "        seg_path = os.path.join(audio_dir, seg_name)\n",
    "        torchaudio.save(seg_path, seg, sr)\n",
    "\n",
    "        # frame at middle of window (i*2s + 1.0s)\n",
    "        ts = (i * chunk_sec) + (chunk_sec / 2.0)\n",
    "        frame_name = f\"{base}_frame{i}.jpg\"\n",
    "        frame_path = os.path.join(image_dir, frame_name)\n",
    "        extract_frame(video_path, frame_path, ts)\n",
    "\n",
    "    print(f\"[INFO] {base}: wrote {n_chunks} segments\")\n",
    "\n",
    "def prepare_dataset_from_videos(raw_videos=RAW_VIDEOS):\n",
    "    video_files = []\n",
    "    video_files.extend(glob.glob(os.path.join(raw_videos, \"*.MOV\")))\n",
    "    video_files.extend(glob.glob(os.path.join(raw_videos, \"*.mov\")))\n",
    "    video_files.extend(glob.glob(os.path.join(raw_videos, \"*.mp4\")))\n",
    "    print(video_files)\n",
    "    if not video_files:\n",
    "        print(f\"[WARN] No videos in {raw_videos}\")\n",
    "    for vf in video_files:\n",
    "        split_audio_and_frames(vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE TRANSFORM\n",
    "gray = transforms.Grayscale(num_output_channels=3)\n",
    "transform_image = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    gray,\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# Audio cleaning for a 2s chunk (band-pass + quiet-frame spectral subtraction)\n",
    "SR        = SAMPLE_RATE\n",
    "BAND_LO   = 12000\n",
    "BAND_HI   = 18000\n",
    "NFFT      = 2048\n",
    "HOP       = 512\n",
    "OVERSUB   = 1.2\n",
    "QUIET_PCT = 0.20\n",
    "\n",
    "def _stft(x: torch.Tensor) -> torch.Tensor:\n",
    "    if x.dim() == 2: x = x.squeeze(0)\n",
    "    return torch.stft(x, n_fft=NFFT, hop_length=HOP, win_length=NFFT,\n",
    "                      window=torch.hann_window(NFFT), return_complex=True, center=True)\n",
    "\n",
    "def _istft(S: torch.Tensor, length: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Inverse STFT that accepts complex tensors when supported,\n",
    "    and falls back to view_as_real format otherwise.\n",
    "    Always uses a real-valued Hann window (float32).\n",
    "    \"\"\"\n",
    "    win = torch.hann_window(NFFT, device=S.device, dtype=torch.float32)\n",
    "    try:\n",
    "        y = torch.istft(\n",
    "            S, n_fft=NFFT, hop_length=HOP, win_length=NFFT,\n",
    "            window=win, length=length, center=True\n",
    "        )\n",
    "    except (TypeError, RuntimeError):\n",
    "        y = torch.istft(\n",
    "            torch.view_as_real(S), n_fft=NFFT, hop_length=HOP, win_length=NFFT,\n",
    "            window=win, length=length, center=True\n",
    "        )\n",
    "    return y\n",
    "\n",
    "def bandpass_chunk(waveform: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Band-pass in STFT domain by zeroing bins outside [BAND_LO, BAND_HI].\n",
    "    Version-agnostic; avoids torchaudio biquad/lfilter kernels.\n",
    "    Input:  waveform [1, T] (float32)\n",
    "    Output: [1, T]\n",
    "    \"\"\"\n",
    "    if waveform.dim() == 1:\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "    x = waveform.squeeze(0).to(torch.float32)          # [T]\n",
    "\n",
    "    # STFT\n",
    "    S = torch.stft(x, n_fft=NFFT, hop_length=HOP, win_length=NFFT,\n",
    "                   window=torch.hann_window(NFFT, dtype=torch.float32),\n",
    "                   return_complex=True, center=True)   # [F, T], complex\n",
    "\n",
    "    # Frequency mask\n",
    "    freqs = np.fft.rfftfreq(NFFT, d=1.0/sr)\n",
    "    lo = int(np.searchsorted(freqs, BAND_LO))\n",
    "    hi = int(np.searchsorted(freqs, BAND_HI))\n",
    "    lo = max(lo, 0); hi = min(hi, S.shape[0])\n",
    "\n",
    "    mask = torch.zeros_like(S, dtype=torch.bool)       # [F, T]\n",
    "    mask[lo:hi, :] = True\n",
    "    S_bp = torch.where(mask, S, torch.zeros_like(S))\n",
    "\n",
    "    # iSTFT back to time (compat wrapper)\n",
    "    y = _istft(S_bp, length=x.numel())                 # [T]\n",
    "    y = y - y.mean()\n",
    "    y = torch.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return y.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_subtract_quiet_frames(waveform: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "    x = waveform.squeeze(0)    # [T]\n",
    "    Tlen = x.shape[-1]\n",
    "    S = _stft(x)               # [F, T] complex\n",
    "    Mag = S.abs()\n",
    "    Pow = Mag**2\n",
    "\n",
    "    freqs = np.fft.rfftfreq(NFFT, d=1.0/sr)\n",
    "    lo = int(np.searchsorted(freqs, BAND_LO))\n",
    "    hi = int(np.searchsorted(freqs, BAND_HI))\n",
    "    lo = max(lo, 0); hi = min(hi, Mag.shape[0])\n",
    "\n",
    "    band_pow_per_frame = Pow[lo:hi].mean(dim=0)         # [T_frames]\n",
    "    T_frames = band_pow_per_frame.numel()\n",
    "    k = max(1, int(round(QUIET_PCT * T_frames)))\n",
    "    vals, idxs = torch.topk(-band_pow_per_frame, k)     # quietest frames\n",
    "    quiet_mask = torch.zeros_like(band_pow_per_frame, dtype=torch.bool)\n",
    "    quiet_mask[idxs] = True\n",
    "\n",
    "    Npsd = Pow[:, quiet_mask].mean(dim=1, keepdim=True) # noise PSD\n",
    "    Pclean = torch.clamp(Pow - OVERSUB * Npsd, min=0.0)\n",
    "    Mag_clean = torch.sqrt(Pclean + 1e-12)\n",
    "    S_clean = Mag_clean * torch.exp(1j * S.angle())\n",
    "    y_clean = _istft(S_clean, length=Tlen)              # [T]\n",
    "    y_clean = y_clean - y_clean.mean()\n",
    "    y_clean = torch.nan_to_num(y_clean, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return y_clean.unsqueeze(0)\n",
    "\n",
    "def clean_chunk(waveform: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "    if waveform.dim() == 2 and waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    elif waveform.dim() == 1:\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "    if sr != SR:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, SR)\n",
    "        sr = SR\n",
    "    y_bp = bandpass_chunk(waveform, sr)\n",
    "    y_cl = spectral_subtract_quiet_frames(y_bp, sr)\n",
    "    return y_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET\n",
    "class AudioImageDataset(Dataset):\n",
    "    def __init__(self, audio_dir, image_dir, label_map, transform_image=None,\n",
    "                 sample_rate=SAMPLE_RATE, n_mels=N_MELS, use_filters=True):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.label_map = label_map\n",
    "        self.transform_image = transform_image\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.use_filters = use_filters\n",
    "\n",
    "        self.audio_files, self.labels, self.video_ids = [], [], []\n",
    "        for file in os.listdir(audio_dir):\n",
    "            if file.endswith(\".wav\") and \"_seg\" in file:\n",
    "                frame_file = file.replace(\"seg\", \"frame\").replace(\".wav\", \".jpg\")\n",
    "                if not os.path.exists(os.path.join(image_dir, frame_file)):\n",
    "                    continue\n",
    "                base_name = file.split(\"_seg\")[0]                \n",
    "                label_str = base_name.split(\"_\")[0].lower()\n",
    "                if label_str in label_map:\n",
    "                    self.audio_files.append(file)                 \n",
    "                    self.labels.append(label_map[label_str])\n",
    "                    self.video_ids.append(file)                   \n",
    "\n",
    "        # Audio -> Mel\n",
    "        self.mel = T.MelSpectrogram(sample_rate=self.sample_rate, n_fft=1024,\n",
    "                                    hop_length=512, n_mels=self.n_mels)\n",
    "        self.db  = T.AmplitudeToDB()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file = self.audio_files[idx]         \n",
    "        label      = self.labels[idx]\n",
    "        seg_id     = self.video_ids[idx]\n",
    "\n",
    "        # ---- AUDIO ----\n",
    "        a_path = os.path.join(self.audio_dir, audio_file)\n",
    "        waveform, sr = torchaudio.load(a_path)     # 2s chunk\n",
    "        if self.use_filters:\n",
    "            waveform = clean_chunk(waveform, sr)   # [1, T] @ 48k\n",
    "        else:\n",
    "            if sr != self.sample_rate:\n",
    "                waveform = torchaudio.functional.resample(waveform, sr, self.sample_rate)\n",
    "\n",
    "        spec = self.mel(waveform)                  # [1, n_mels, time]\n",
    "        spec = self.db(spec)\n",
    "        spec = (spec - spec.mean()) / (spec.std() + 1e-6)\n",
    "        spec = F.interpolate(spec.unsqueeze(0), size=(224,224), mode=\"bilinear\", align_corners=False)\n",
    "        spec = spec.mean(dim=1)                    # [1,224,224]\n",
    "\n",
    "        # ---- IMAGE ----\n",
    "        frame_file = audio_file.replace(\"seg\", \"frame\").replace(\".wav\", \".jpg\")\n",
    "        img_path = os.path.join(self.image_dir, frame_file)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform_image:\n",
    "            img = self.transform_image(img)\n",
    "\n",
    "        return spec, img, label, seg_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cobas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
