{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. *Band-pass Filter Parameters*\n",
    "- `BAND_LO` and `BAND_HI`: Adjust frequency range (currently 12-18 kHz)\n",
    "- Target different frequency bands for different battery states\n",
    "\n",
    "    - This is essentially how we filter only the frequencies within our range of interest (the ultrasonic waves)\n",
    "\n",
    "## 2. *Spectral Subtraction Parameters*\n",
    "- `QUIET_PCT`: Percentage of quietest frames for noise estimation (currently 20%)\n",
    "- `OVERSUB`: Over-subtraction factor (currently 1.2)\n",
    "    - used to remove noise from our recordings\n",
    "\n",
    "## 3. **STFT Parameters**\n",
    "- `NFFT`: FFT size affects frequency resolution (currently 2048)\n",
    "- `HOP`: Hop length affects time resolution (currently 512)\n",
    "\n",
    "    - an STFT is basically just a time frequency domain representation of the audio waveform, instead of showing up in a magnitude format. \n",
    "\n",
    "## 4. **Mel-spectrogram Parameters**\n",
    "- `N_MELS`: Number of mel bins (currently 64)\n",
    "- `n_fft`: FFT size for mel transform (currently 1024)\n",
    "\n",
    "    - Mel-spectrograms are a way to represent audio signals in a way that aligns with how we perceive sound, specifically focusing on freqs that we can hear more distinctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, subprocess, math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.functional as AF\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# SETTINGS\n",
    "RAW_VIDEOS   = \"raw_videos\"     # input videos\n",
    "AUDIO_DIR    = \"audio_segments\"   # output 2s wavs\n",
    "IMAGE_DIR    = \"recorded_images\"  # output frame jpgs\n",
    "\n",
    "SAMPLE_RATE  = 48000       \n",
    "CHUNK_SECONDS= 2\n",
    "N_MELS       = 64\n",
    "BATCH_SIZE   = 20\n",
    "EPOCHS       = 15\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "label_map = {\"full\": 0, \"half\": 1, \"empty\": 2}\n",
    "LABELS = {v: k.capitalize() for k, v in label_map.items()}\n",
    "\n",
    "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT 2s AUDIO + FRAMES FROM VIDEO\n",
    "def extract_audio(video_path, wav_out, sample_rate=SAMPLE_RATE):\n",
    "    # Use ffmpeg to extract audio from the video file\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-y\", \"-i\", video_path,  # Input video file\n",
    "        \"-ar\", str(sample_rate), \"-ac\", \"1\", wav_out  # Output audio file with specified sample rate and mono channel\n",
    "    ]\n",
    "    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)  # Run the command silently\n",
    "\n",
    "def extract_frame(video_path, image_out, timestamp):\n",
    "    # Use ffmpeg to extract a single frame from the video at a specific timestamp\n",
    "    cmd = [\"ffmpeg\", \"-y\", \"-i\", video_path, \"-ss\", f\"{timestamp:.3f}\", \"-vframes\", \"1\", image_out]\n",
    "    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)  # Run the command silently\n",
    "\n",
    "def split_audio_and_frames(video_path, audio_dir=AUDIO_DIR, image_dir=IMAGE_DIR,\n",
    "                           chunk_sec=CHUNK_SECONDS, sample_rate=SAMPLE_RATE):\n",
    "    # Extract the base name of the video file (e.g., full_8)\n",
    "    base = os.path.splitext(os.path.basename(video_path))[0]\n",
    "\n",
    "    # 1) Extract full audio from the video\n",
    "    full_audio = os.path.join(audio_dir, base + \".wav\")  # Path for the extracted audio file\n",
    "    extract_audio(video_path, full_audio, sample_rate=sample_rate)  # Extract audio\n",
    "\n",
    "    # 2) Load the extracted audio and split it into 2-second chunks\n",
    "    waveform, sr = torchaudio.load(full_audio)  # Load the audio waveform and sample rate\n",
    "    if sr != sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, sample_rate)  # Resample if needed\n",
    "        sr = sample_rate\n",
    "\n",
    "    total_samples = waveform.shape[-1]  # Total number of samples in the audio\n",
    "    chunk_samples = int(chunk_sec * sr)  # Number of samples per chunk\n",
    "    n_chunks = total_samples // chunk_samples  # Total number of chunks\n",
    "\n",
    "    for i in range(n_chunks):\n",
    "        # Extract a chunk of audio\n",
    "        start = i * chunk_samples  # Start sample index\n",
    "        end   = start + chunk_samples  # End sample index\n",
    "        seg   = waveform[:, start:end]  # Extract the segment\n",
    "        seg_name = f\"{base}_seg{i}.wav\"  # Name for the audio segment file\n",
    "        seg_path = os.path.join(audio_dir, seg_name)  # Path for the audio segment file\n",
    "        torchaudio.save(seg_path, seg, sr)  # Save the audio segment\n",
    "\n",
    "        # Extract the frame at the middle of the audio chunk (i*2s + 1.0s)\n",
    "        ts = (i * chunk_sec) + (chunk_sec / 2.0)  # Timestamp for the frame\n",
    "        frame_name = f\"{base}_frame{i}.jpg\"  # Name for the frame image file\n",
    "        frame_path = os.path.join(image_dir, frame_name)  # Path for the frame image file\n",
    "        extract_frame(video_path, frame_path, ts)  # Extract the frame\n",
    "\n",
    "    print(f\"[INFO] {base}: wrote {n_chunks} segments\")  # Log the number of segments created\n",
    "\n",
    "def prepare_dataset_from_videos(raw_videos=RAW_VIDEOS):\n",
    "    # Collect all video files from the raw_videos directory\n",
    "    video_files = []\n",
    "    video_files.extend(glob.glob(os.path.join(raw_videos, \"*.MOV\")))  # Add .MOV files\n",
    "    video_files.extend(glob.glob(os.path.join(raw_videos, \"*.mov\")))  # Add .mov files\n",
    "    video_files.extend(glob.glob(os.path.join(raw_videos, \"*.mp4\")))  # Add .mp4 files\n",
    "\n",
    "    print(video_files)  # Print the list of video files found\n",
    "    if not video_files:\n",
    "        print(f\"[WARN] No videos in {raw_videos}\")  # Warn if no videos are found\n",
    "\n",
    "    # Process each video file to extract audio and frames\n",
    "    for vf in video_files:\n",
    "        split_audio_and_frames(vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE TRANSFORM\n",
    "gray = transforms.Grayscale(num_output_channels=3)\n",
    "transform_image = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    gray,\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# Audio cleaning for a 2s chunk (band-pass + quiet-frame spectral subtraction)\n",
    "SR        = SAMPLE_RATE\n",
    "BAND_LO   = 12000\n",
    "BAND_HI   = 18000\n",
    "NFFT      = 2048\n",
    "HOP       = 512\n",
    "OVERSUB   = 1.2\n",
    "QUIET_PCT = 0.20\n",
    "\n",
    "def _stft(x: torch.Tensor) -> torch.Tensor:\n",
    "    if x.dim() == 2: x = x.squeeze(0)\n",
    "    return torch.stft(x, n_fft=NFFT, hop_length=HOP, win_length=NFFT,\n",
    "                      window=torch.hann_window(NFFT), return_complex=True, center=True)\n",
    "\n",
    "def _istft(S: torch.Tensor, length: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Inverse STFT that accepts complex tensors when supported,\n",
    "    and falls back to view_as_real format otherwise.\n",
    "    Always uses a real-valued Hann window (float32).\n",
    "    \"\"\"\n",
    "    win = torch.hann_window(NFFT, device=S.device, dtype=torch.float32)\n",
    "    try:\n",
    "        y = torch.istft(\n",
    "            S, n_fft=NFFT, hop_length=HOP, win_length=NFFT,\n",
    "            window=win, length=length, center=True\n",
    "        )\n",
    "    except (TypeError, RuntimeError):\n",
    "        y = torch.istft(\n",
    "            torch.view_as_real(S), n_fft=NFFT, hop_length=HOP, win_length=NFFT,\n",
    "            window=win, length=length, center=True\n",
    "        )\n",
    "    return y\n",
    "\n",
    "def bandpass_chunk(waveform: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Band-pass in STFT domain by zeroing bins outside [BAND_LO, BAND_HI].\n",
    "    Version-agnostic; avoids torchaudio biquad/lfilter kernels.\n",
    "    Input:  waveform [1, T] (float32)\n",
    "    Output: [1, T]\n",
    "    \"\"\"\n",
    "    if waveform.dim() == 1:\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "    x = waveform.squeeze(0).to(torch.float32)          # [T]\n",
    "\n",
    "    # STFT\n",
    "    S = torch.stft(x, n_fft=NFFT, hop_length=HOP, win_length=NFFT,\n",
    "                   window=torch.hann_window(NFFT, dtype=torch.float32),\n",
    "                   return_complex=True, center=True)   # [F, T], complex\n",
    "\n",
    "    # Frequency mask\n",
    "    freqs = np.fft.rfftfreq(NFFT, d=1.0/sr)\n",
    "    lo = int(np.searchsorted(freqs, BAND_LO))\n",
    "    hi = int(np.searchsorted(freqs, BAND_HI))\n",
    "    lo = max(lo, 0); hi = min(hi, S.shape[0])\n",
    "\n",
    "    mask = torch.zeros_like(S, dtype=torch.bool)       # [F, T]\n",
    "    mask[lo:hi, :] = True\n",
    "    S_bp = torch.where(mask, S, torch.zeros_like(S))\n",
    "\n",
    "    # iSTFT back to time (compat wrapper)\n",
    "    y = _istft(S_bp, length=x.numel())                 # [T]\n",
    "    y = y - y.mean()\n",
    "    y = torch.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return y.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_subtract_quiet_frames(waveform: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "    # Squeeze the waveform to remove unnecessary dimensions, resulting in a 1D tensor [T]\n",
    "    x = waveform.squeeze(0)\n",
    "    Tlen = x.shape[-1]  # Get the total length of the waveform\n",
    "\n",
    "    # Perform Short-Time Fourier Transform (STFT) to convert the waveform to the frequency domain\n",
    "    S = _stft(x)  # [F, T] complex tensor where F is frequency bins and T is time frames\n",
    "    Mag = S.abs()  # Magnitude of the STFT\n",
    "    Pow = Mag**2  # Power spectrum\n",
    "\n",
    "    # Define the frequency range of interest (band-pass filter)\n",
    "    freqs = np.fft.rfftfreq(NFFT, d=1.0/sr)  # Generate frequency bins\n",
    "    lo = int(np.searchsorted(freqs, BAND_LO))  # Find the lower bound index for the band\n",
    "    hi = int(np.searchsorted(freqs, BAND_HI))  # Find the upper bound index for the band\n",
    "    lo = max(lo, 0)  # Ensure the lower bound is within range\n",
    "    hi = min(hi, Mag.shape[0])  # Ensure the upper bound is within range\n",
    "\n",
    "    # Calculate the average power within the band for each time frame\n",
    "    band_pow_per_frame = Pow[lo:hi].mean(dim=0)  # [T_frames]\n",
    "    T_frames = band_pow_per_frame.numel()  # Total number of time frames\n",
    "\n",
    "    # Identify the quietest frames based on the specified percentage (QUIET_PCT)\n",
    "    k = max(1, int(round(QUIET_PCT * T_frames)))  # Number of quietest frames to select\n",
    "    vals, idxs = torch.topk(-band_pow_per_frame, k)  # Get indices of the quietest frames\n",
    "    quiet_mask = torch.zeros_like(band_pow_per_frame, dtype=torch.bool)  # Initialize mask\n",
    "    quiet_mask[idxs] = True  # Mark the quietest frames in the mask\n",
    "\n",
    "    # Estimate the noise power spectral density (PSD) from the quiet frames\n",
    "    Npsd = Pow[:, quiet_mask].mean(dim=1, keepdim=True)  # Noise PSD\n",
    "\n",
    "    # Perform spectral subtraction to remove noise\n",
    "    Pclean = torch.clamp(Pow - OVERSUB * Npsd, min=0.0)  # Cleaned power spectrum\n",
    "    Mag_clean = torch.sqrt(Pclean + 1e-12)  # Cleaned magnitude spectrum\n",
    "\n",
    "    # Reconstruct the cleaned STFT using the original phase information\n",
    "    S_clean = Mag_clean * torch.exp(1j * S.angle())\n",
    "\n",
    "    # Perform inverse STFT to convert back to the time domain\n",
    "    y_clean = _istft(S_clean, length=Tlen)  # [T]\n",
    "\n",
    "    # Remove DC offset and handle any NaN or infinite values\n",
    "    y_clean = y_clean - y_clean.mean()\n",
    "    y_clean = torch.nan_to_num(y_clean, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # Return the cleaned waveform as a 2D tensor [1, T]\n",
    "    return y_clean.unsqueeze(0)\n",
    "\n",
    "def clean_chunk(waveform: torch.Tensor, sr: int) -> torch.Tensor:\n",
    "    # Ensure the waveform is a 2D tensor [1, T] by averaging or unsqueezing if necessary\n",
    "    if waveform.dim() == 2 and waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # Average across channels\n",
    "    elif waveform.dim() == 1:\n",
    "        waveform = waveform.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "    # Resample the waveform to the target sample rate (SR) if needed\n",
    "    if sr != SR:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, SR)\n",
    "        sr = SR\n",
    "\n",
    "    # Apply band-pass filtering to isolate the frequency range of interest\n",
    "    y_bp = bandpass_chunk(waveform, sr)\n",
    "\n",
    "    # Perform spectral subtraction to remove noise from the filtered waveform\n",
    "    y_cl = spectral_subtract_quiet_frames(y_bp, sr)\n",
    "\n",
    "    # Return the cleaned waveform\n",
    "    return y_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET\n",
    "class AudioImageDataset(Dataset):\n",
    "    def __init__(self, audio_dir, image_dir, label_map, transform_image=None,\n",
    "                 sample_rate=SAMPLE_RATE, n_mels=N_MELS, use_filters=True):\n",
    "        # Initialize dataset parameters\n",
    "        self.audio_dir = audio_dir  # Directory containing audio files\n",
    "        self.image_dir = image_dir  # Directory containing image files\n",
    "        self.label_map = label_map  # Mapping of labels to integers\n",
    "        self.transform_image = transform_image  # Image transformation pipeline\n",
    "        self.sample_rate = sample_rate  # Target sample rate for audio\n",
    "        self.n_mels = n_mels  # Number of mel bins for spectrogram\n",
    "        self.use_filters = use_filters  # Whether to apply audio cleaning filters\n",
    "\n",
    "        # Initialize lists to store dataset information\n",
    "        self.audio_files, self.labels, self.video_ids = [], [], []\n",
    "        for file in os.listdir(audio_dir):  # Iterate through audio directory\n",
    "            if file.endswith(\".wav\") and \"_seg\" in file:  # Check for segmented audio files\n",
    "                frame_file = file.replace(\"seg\", \"frame\").replace(\".wav\", \".jpg\")  # Corresponding image file\n",
    "                if not os.path.exists(os.path.join(image_dir, frame_file)):\n",
    "                    continue  # Skip if corresponding image file does not exist\n",
    "                base_name = file.split(\"_seg\")[0]  # Extract base name of the file\n",
    "                label_str = base_name.split(\"_\")[0].lower()  # Extract label from file name\n",
    "                if label_str in label_map:  # Check if label is valid\n",
    "                    self.audio_files.append(file)  # Add audio file to list\n",
    "                    self.labels.append(label_map[label_str])  # Add label to list\n",
    "                    self.video_ids.append(file)  # Add video ID to list\n",
    "\n",
    "        # Define audio transformations: Mel-spectrogram and amplitude-to-decibel conversion\n",
    "        self.mel = T.MelSpectrogram(sample_rate=self.sample_rate, n_fft=1024,\n",
    "                                    hop_length=512, n_mels=self.n_mels)\n",
    "        self.db  = T.AmplitudeToDB()  # Convert amplitude to decibels\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the audio file, label, and video ID for the given index\n",
    "        audio_file = self.audio_files[idx]  # Audio file name\n",
    "        label      = self.labels[idx]  # Corresponding label\n",
    "        seg_id     = self.video_ids[idx]  # Segment ID\n",
    "\n",
    "        # ---- AUDIO ----\n",
    "        a_path = os.path.join(self.audio_dir, audio_file)  # Full path to audio file\n",
    "        waveform, sr = torchaudio.load(a_path)  # Load audio waveform and sample rate\n",
    "        if self.use_filters:\n",
    "            waveform = clean_chunk(waveform, sr)  # Apply audio cleaning filters\n",
    "        else:\n",
    "            if sr != self.sample_rate:\n",
    "                waveform = torchaudio.functional.resample(waveform, sr, self.sample_rate)  # Resample audio\n",
    "\n",
    "        # Convert waveform to mel-spectrogram\n",
    "        spec = self.mel(waveform)  # [1, n_mels, time]\n",
    "        spec = self.db(spec)  # Convert to decibel scale\n",
    "        spec = (spec - spec.mean()) / (spec.std() + 1e-6)  # Normalize spectrogram\n",
    "        spec = F.interpolate(spec.unsqueeze(0), size=(224,224), mode=\"bilinear\", align_corners=False)  # Resize to 224x224\n",
    "        spec = spec.mean(dim=1)  # Reduce to single channel [1,224,224]\n",
    "\n",
    "        # ---- IMAGE ----\n",
    "        frame_file = audio_file.replace(\"seg\", \"frame\").replace(\".wav\", \".jpg\")  # Corresponding image file name\n",
    "        img_path = os.path.join(self.image_dir, frame_file)  # Full path to image file\n",
    "        img = Image.open(img_path).convert(\"RGB\")  # Load image and convert to RGB\n",
    "        if self.transform_image:\n",
    "            img = self.transform_image(img)  # Apply image transformations\n",
    "\n",
    "        # Return the processed spectrogram, image, label, and segment ID\n",
    "        return spec, img, label, seg_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cobas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
